{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1\"><a href=\"#Task-1.-Compiling-Ebola-Data\"><span class=\"toc-item-num\">Task 1.&nbsp;&nbsp;</span>Compiling Ebola Data</a></div>\n",
    " <div class=\"lev1\"><a href=\"#Task-2.-RNA-Sequences\"><span class=\"toc-item-num\">Task 2.&nbsp;&nbsp;</span>RNA Sequences</a></div>\n",
    " <div class=\"lev1\"><a href=\"#Task-3.-Class-War-in-Titanic\"><span class=\"toc-item-num\">Task 3.&nbsp;&nbsp;</span>Class War in Titanic</a></div></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = '../../Data' # Use the data folder provided in Tutorial 02 - Intro to Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Compiling Ebola Data\n",
    "\n",
    "The `DATA_FOLDER/ebola` folder contains summarized reports of Ebola cases from three countries (Guinea, Liberia and Sierra Leone) during the recent outbreak of the disease in West Africa. For each country, there are daily reports that contain various information about the outbreak in several cities in each country.\n",
    "\n",
    "Use pandas to import these data files into a single `Dataframe`.\n",
    "Using this `DataFrame`, calculate for *each country*, the *daily average* per year of *new cases* and *deaths*.\n",
    "Make sure you handle all the different expressions for *new cases* and *deaths* that are used in the reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your answer here\n",
    "#first We need to import the data as a dataframe of course it means importing pandas\n",
    "#create a unique dataframe with country:date::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import glob\n",
    "import math \n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Create a column with name of the country in the beginning of the frame\n",
    "def add_pays(data_frame, pays):\n",
    "    data_frame.loc[:,'Pays'] = pd.Series(pays, index=data_frame.index)\n",
    "    cols = list(data_frame)\n",
    "    cols.insert(0, cols.pop(cols.index('Pays')))\n",
    "    data_frame = data_frame.loc[:, cols]\n",
    "    return data_frame\n",
    "\n",
    "#import the csv files of one country and put them in one frame\n",
    "def create_oneframe(pays):\n",
    "    allFiles = glob.glob(DATA_FOLDER + \"/ebola/%s_data/*.csv\"%pays)\n",
    "    list_ = []\n",
    "    frame=pd.DataFrame()\n",
    "    for file_ in allFiles:\n",
    "        df = pd.read_csv(file_,)\n",
    "        if not frame.empty:\n",
    "            frame = pd.merge(frame, df, how='outer')\n",
    "        else :\n",
    "            frame = df\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pays1='guinea'\n",
    "pays2='liberia'\n",
    "pays3='sl'   #Sierra Leone\n",
    "\n",
    "guinea=add_pays(create_oneframe(pays1),(pays1))\n",
    "liberia=add_pays(create_oneframe(pays2),(pays2))\n",
    "sl=add_pays(create_oneframe(pays3),(pays3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_country=pd.DataFrame()\n",
    "all_country=pd.merge(sl,guinea,how='outer')\n",
    "all_country=pd.merge(all_country,liberia,how='outer')\n",
    "\n",
    "##create a new column for country that dont have one. guinea:Totals, liberia:National\n",
    "## and a new date and new description\n",
    "#all_country=all_country.fillna('')\n",
    "all_country[\"Description\"] = all_country[\"Description\"].fillna('').map(str) + all_country[\"Variable\"].fillna('').map(str)+all_country[\"variable\"].fillna('').map(str)\n",
    "all_country[\"Date\"] = all_country[\"Date\"].fillna('').map(str) + all_country[\"date\"].fillna('').map(str)\n",
    "all_country['National']=all_country['National'].fillna('').map(str)+all_country['Totals'].fillna('').map(str)\n",
    "\n",
    "##Delete the old ones\n",
    "cols = list(all_country)\n",
    "cols.pop(cols.index('Variable'))\n",
    "cols.pop(cols.index('variable'))\n",
    "cols.pop(cols.index('date'))\n",
    "cols.pop(cols.index('Totals'))\n",
    "cols.insert(1, cols.pop(cols.index('Date')))\n",
    "cols.insert(2, cols.pop(cols.index('Description')))\n",
    "cols.insert(3, cols.pop(cols.index('National')))\n",
    "all_country = all_country.loc[:, cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: search for missing values in National: if empty sum following columns\n",
    "###Pour remplir les totaux vides\n",
    "Natio_empty=np.where((all_country.National==''))[0]\n",
    "\n",
    "\n",
    "all_country.replace({'-':0},inplace=True)\n",
    "for i in all_country.index[Natio_empty]:\n",
    "    #Due to the impossibility to delete % We chose to avoid those lines\n",
    "    if  i!=818 and i!=1150:\n",
    "        all_country.iloc[i,3]=pd.Series.sum(all_country.iloc[i,4:].astype(float))\n",
    "\n",
    "display(all_country.iloc[(all_country.index[Natio_empty])].head())\n",
    "\n",
    "###or delete those rows \n",
    "\n",
    "#print(len(Natio_empty))\n",
    "#print(len(all_country))\n",
    "#all_country.drop(all_country.index[Natio_empty],inplace=True)\n",
    "\n",
    "### The rows deleted are not important for the main Description\n",
    "\n",
    "#display(all_country.index[Natio_empty])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now we have a dataframe with which we can work. \n",
    "#The goal here is to calculate the daily average of each description since it is not clear which one is necessary\n",
    "\n",
    "\n",
    "countries=['guinea','liberia','sl']\n",
    "\n",
    "#create new column with our daily average\n",
    "all_country['Daily Average']=np.nan\n",
    "cols = list(all_country)\n",
    "cols.insert(3, cols.pop(cols.index('Daily Average')))\n",
    "all_country = all_country.loc[:, cols]\n",
    "\n",
    "for country in countries:\n",
    "    #work with dataframe of the country\n",
    "    one_country=all_country[all_country.Pays.str.startswith(country)]\n",
    "    descriptions=one_country.Description.unique()\n",
    "    for desc in (descriptions[5:6]):\n",
    "        one_description=one_country.loc[one_country.Description==desc,:]\n",
    "        #does not work very weird:\n",
    "        #one_description=one_country.loc[one_country.Description.str.contains('New case/s con')]\n",
    "        if (True in one_description['National'].str.contains(',').values+\\\n",
    "            one_description['National'].str.contains('%').values):\n",
    "            print('description rejected:%s'%(desc))\n",
    "        else: \n",
    "            descr_mean=(one_description['National']).astype(float).mean()\n",
    "            if ((\"new\" not in desc) and ('New' not in desc)) and len(one_description['National'])!=0:\n",
    "                descr_mean=max(one_description['National'].astype(float))/len(one_description['National'])\n",
    "            #print(descr_mean)\n",
    "            all_country.loc[(all_country.Pays==country) \\\n",
    "                            & (all_country.Description==desc),'Daily Average']=descr_mean\n",
    "        \n",
    "        #all_country[(all_country.Pays.any()==country) and (all_country.Description==desc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now for anytype of Description it is possible to have the daily average easily:\n",
    "\n",
    "descr_dict={}\n",
    "description='new_contacts'\n",
    "for country in countries:\n",
    "    average=all_country.loc[(all_country.Pays==country)& (all_country.Description.str.contains(description)),:]['Daily Average'].unique()\n",
    "    descr_dict[country]=average\n",
    "print('For the description:\"%s\" \\n %s'%(description,descr_dict))\n",
    "\n",
    "#or just plot the full data frame for a specific country and description\n",
    "display(all_country.loc[(all_country.Pays=='liberia') & (all_country.Description.str.contains('New')),:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here the new death confirmed and new cases confirmed  are calculated for each country in a slightly different way\n",
    "death_dict={}\n",
    "newcase_dict={}\n",
    "#Boolean to print the evolution of the new death confirmed and new cases confirmed \n",
    "print_evolution=False\n",
    "for country in countries:\n",
    "\n",
    "    one_country=all_country[all_country.Pays.str.startswith(country)]\n",
    "\n",
    "    #frame with only rows of new cases confirmed:\n",
    "    country_new=one_country[one_country.Description.str.contains('New',case=False)& (one_country.Description.str.contains('confirmed'))\\\n",
    "                             &(~one_country.Description.str.contains('Death',case=False))&(~one_country.Description.str.contains('workers',case=False))]\n",
    "    #average over the columns\n",
    "    #news cases confirmed daily average over the year\n",
    "    newcase_dict[country]=(country_new.National.astype(float).mean())\n",
    "    #frame with only rows of death cases confirmed:\n",
    "    country_death=one_country[one_country.Description.str.contains('deaths',case=False) & ~(one_country.Description.str.contains('HCW'))\\\n",
    "                             &(one_country.Description.str.contains('new',case=False)) &~(one_country.Description.str.contains('proba'))\\\n",
    "                             &~(one_country.Description.str.contains('workers'))&~(one_country.Description.str.contains('confirmed'))]\n",
    "    #daily death average over year :\n",
    "    death_dict[country]=(country_death.National.astype(float).mean())\n",
    "    \n",
    "    ###plot of the new cases:\n",
    "    date=country_death.Date.tolist()\n",
    "    date=[parse(x) for x in date]\n",
    "    newcase=country_new.National.astype(float).tolist()\n",
    "    date, newcase = zip(*sorted(zip(date, newcase)))\n",
    "    \n",
    "    \n",
    "    ###plot of the death everyday (in increasing date)\n",
    "    date=country_death.Date.tolist()\n",
    "    date=[parse(x) for x in date]\n",
    "    death=country_death.National.astype(float).tolist()\n",
    "    date, death = zip(*sorted(zip(date, death)))\n",
    "    if print_evolution:\n",
    "\n",
    "        plt.plot(newcase)\n",
    "        plt.title(country)\n",
    "        plt.xlabel('Days')\n",
    "        plt.ylabel('New cases')\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(death)\n",
    "        plt.title(country)\n",
    "        plt.xlabel('Days')\n",
    "        plt.ylabel('New Deaths')\n",
    "        plt.show()\n",
    "\n",
    "print('New cases average:%s'%newcase_dict)\n",
    "print('death average:%s'%death_dict)\n",
    "print('Libera was the most affected country')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. RNA Sequences\n",
    "\n",
    "In the `DATA_FOLDER/microbiome` subdirectory, there are 9 spreadsheets of microbiome data that was acquired from high-throughput RNA sequencing procedures, along with a 10<sup>th</sup> file that describes the content of each. \n",
    "\n",
    "Use pandas to import the first 9 spreadsheets into a single `DataFrame`.\n",
    "Then, add the metadata information from the 10<sup>th</sup> spreadsheet as columns in the combined `DataFrame`.\n",
    "Make sure that the final `DataFrame` has a unique index and all the `NaN` values have been replaced by the tag `unknown`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import necessary modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loading the data for each MID file\n",
    "microbiomePath = '../../Data/microbiome/MID'\n",
    "\n",
    "for x in range(1, 10):\n",
    "    path = \"../../Data/microbiome/MID#.xls\"\n",
    "    path = path.replace('#',str(x))\n",
    "    globals()['mb%s' % x] = pd.read_excel( path, header=None, index_col=0)\n",
    "   # print(globals()['mb%s' % x].head())\n",
    "#Loading Metadata as DataFrame \n",
    "\n",
    "metaData = pd.read_excel('../../Data/microbiome/metadata.xls')\n",
    "metaData.loc[0,'SAMPLE'] = 'unknown'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We create a list of DataFrames to facilitate the manipulation of the DataFrames\n",
    "mbArray = [mb1,mb2,mb3,mb4,mb5,mb6,mb7,mb8,mb9]\n",
    "count = 0;\n",
    "# Adding Group and Sample for each microbiome DataFrame with the expected values from the metaData \n",
    "for x in mbArray :\n",
    "    Group = np.repeat(metaData.loc[count,'GROUP'],len(x.index))\n",
    "    Sample = np.repeat(metaData.loc[count,'SAMPLE'],len(x.index))\n",
    "    x.columns = ['Values']\n",
    "    x['Group'] = pd.Series(Group, index=x.index)\n",
    "    x['Sample'] = pd.Series(Sample, index=x.index)\n",
    "    x.index.name = 'Taxon'\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating an array with all the dataFrames\n",
    "originalTables = []\n",
    "for x in range (1,10):\n",
    "    key = 'MID#'\n",
    "    key = key.replace('#',str(x))\n",
    "    originalTables.append(key)\n",
    "    \n",
    "finalMicrobiome = pd.concat(mbArray, keys=originalTables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "finalMicrobiome.index.is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalMicrobiome[::30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Class War in Titanic\n",
    "\n",
    "Use pandas to import the data file `Data/titanic.xls`. It contains data on all the passengers that travelled on the Titanic."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(filename= DATA_FOLDER +'/titanic.html')"
=======
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>survived</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>ticket</th>\n",
       "      <th>fare</th>\n",
       "      <th>cabin</th>\n",
       "      <th>embarked</th>\n",
       "      <th>boat</th>\n",
       "      <th>body</th>\n",
       "      <th>home.dest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Allen, Miss. Elisabeth Walton</td>\n",
       "      <td>female</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24160</td>\n",
       "      <td>211.3375</td>\n",
       "      <td>B5</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>St Louis, MO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Allison, Master. Hudson Trevor</td>\n",
       "      <td>male</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Allison, Miss. Helen Loraine</td>\n",
       "      <td>female</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Allison, Mr. Hudson Joshua Creighton</td>\n",
       "      <td>male</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>135.0</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Allison, Mrs. Hudson J C (Bessie Waldo Daniels)</td>\n",
       "      <td>female</td>\n",
       "      <td>25.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Anderson, Mr. Harry</td>\n",
       "      <td>male</td>\n",
       "      <td>48.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19952</td>\n",
       "      <td>26.5500</td>\n",
       "      <td>E12</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New York, NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Andrews, Miss. Kornelia Theodosia</td>\n",
       "      <td>female</td>\n",
       "      <td>63.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13502</td>\n",
       "      <td>77.9583</td>\n",
       "      <td>D7</td>\n",
       "      <td>S</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hudson, NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Andrews, Mr. Thomas Jr</td>\n",
       "      <td>male</td>\n",
       "      <td>39.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112050</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>A36</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Belfast, NI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Appleton, Mrs. Edward Dale (Charlotte Lamson)</td>\n",
       "      <td>female</td>\n",
       "      <td>53.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>11769</td>\n",
       "      <td>51.4792</td>\n",
       "      <td>C101</td>\n",
       "      <td>S</td>\n",
       "      <td>D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bayside, Queens, NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Artagaveytia, Mr. Ramon</td>\n",
       "      <td>male</td>\n",
       "      <td>71.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17609</td>\n",
       "      <td>49.5042</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.0</td>\n",
       "      <td>Montevideo, Uruguay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Astor, Col. John Jacob</td>\n",
       "      <td>male</td>\n",
       "      <td>47.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17757</td>\n",
       "      <td>227.5250</td>\n",
       "      <td>C62 C64</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>124.0</td>\n",
       "      <td>New York, NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Astor, Mrs. John Jacob (Madeleine Talmadge Force)</td>\n",
       "      <td>female</td>\n",
       "      <td>18.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17757</td>\n",
       "      <td>227.5250</td>\n",
       "      <td>C62 C64</td>\n",
       "      <td>C</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New York, NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Aubart, Mme. Leontine Pauline</td>\n",
       "      <td>female</td>\n",
       "      <td>24.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17477</td>\n",
       "      <td>69.3000</td>\n",
       "      <td>B35</td>\n",
       "      <td>C</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Paris, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Barber, Miss. Ellen \"Nellie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19877</td>\n",
       "      <td>78.8500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Barkworth, Mr. Algernon Henry Wilson</td>\n",
       "      <td>male</td>\n",
       "      <td>80.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27042</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>A23</td>\n",
       "      <td>S</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hessle, Yorks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Baumann, Mr. John D</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17318</td>\n",
       "      <td>25.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New York, NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Baxter, Mr. Quigg Edmond</td>\n",
       "      <td>male</td>\n",
       "      <td>24.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PC 17558</td>\n",
       "      <td>247.5208</td>\n",
       "      <td>B58 B60</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montreal, PQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Baxter, Mrs. James (Helene DeLaudeniere Chaput)</td>\n",
       "      <td>female</td>\n",
       "      <td>50.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PC 17558</td>\n",
       "      <td>247.5208</td>\n",
       "      <td>B58 B60</td>\n",
       "      <td>C</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montreal, PQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Bazzani, Miss. Albina</td>\n",
       "      <td>female</td>\n",
       "      <td>32.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11813</td>\n",
       "      <td>76.2917</td>\n",
       "      <td>D15</td>\n",
       "      <td>C</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Beattie, Mr. Thomson</td>\n",
       "      <td>male</td>\n",
       "      <td>36.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13050</td>\n",
       "      <td>75.2417</td>\n",
       "      <td>C6</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Winnipeg, MN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Beckwith, Mr. Richard Leonard</td>\n",
       "      <td>male</td>\n",
       "      <td>37.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11751</td>\n",
       "      <td>52.5542</td>\n",
       "      <td>D35</td>\n",
       "      <td>S</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New York, NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Beckwith, Mrs. Richard Leonard (Sallie Monypeny)</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11751</td>\n",
       "      <td>52.5542</td>\n",
       "      <td>D35</td>\n",
       "      <td>S</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New York, NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C148</td>\n",
       "      <td>C</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New York, NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Bidois, Miss. Rosalie</td>\n",
       "      <td>female</td>\n",
       "      <td>42.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17757</td>\n",
       "      <td>227.5250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Bird, Miss. Ellen</td>\n",
       "      <td>female</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17483</td>\n",
       "      <td>221.7792</td>\n",
       "      <td>C97</td>\n",
       "      <td>S</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Birnbaum, Mr. Jakob</td>\n",
       "      <td>male</td>\n",
       "      <td>25.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13905</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>148.0</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Bishop, Mr. Dickinson H</td>\n",
       "      <td>male</td>\n",
       "      <td>25.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11967</td>\n",
       "      <td>91.0792</td>\n",
       "      <td>B49</td>\n",
       "      <td>C</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dowagiac, MI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Bishop, Mrs. Dickinson H (Helen Walton)</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11967</td>\n",
       "      <td>91.0792</td>\n",
       "      <td>B49</td>\n",
       "      <td>C</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dowagiac, MI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Bissette, Miss. Amelia</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17760</td>\n",
       "      <td>135.6333</td>\n",
       "      <td>C99</td>\n",
       "      <td>S</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Bjornstrom-Steffansson, Mr. Mauritz Hakan</td>\n",
       "      <td>male</td>\n",
       "      <td>28.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>110564</td>\n",
       "      <td>26.5500</td>\n",
       "      <td>C52</td>\n",
       "      <td>S</td>\n",
       "      <td>D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Stockholm, Sweden / Washington, DC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1279</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Vestrom, Miss. Hulda Amanda Adolfina</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>350406</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1280</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Vovk, Mr. Janko</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349252</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Waelens, Mr. Achille</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>345767</td>\n",
       "      <td>9.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Antwerp, Belgium / Stanton, OH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1282</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Ware, Mr. Frederick</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>359309</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1283</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Warren, Mr. Charles William</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C.A. 49867</td>\n",
       "      <td>7.5500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1284</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Webber, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SOTON/OQ 3101316</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1285</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Wenzel, Mr. Linhart</td>\n",
       "      <td>male</td>\n",
       "      <td>32.5000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>345775</td>\n",
       "      <td>9.5000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>298.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1286</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Whabee, Mrs. George Joseph (Shawneene Abi-Saab)</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2688</td>\n",
       "      <td>7.2292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1287</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Widegren, Mr. Carl/Charles Peter</td>\n",
       "      <td>male</td>\n",
       "      <td>51.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>347064</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1288</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Wiklund, Mr. Jakob Alfred</td>\n",
       "      <td>male</td>\n",
       "      <td>18.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3101267</td>\n",
       "      <td>6.4958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>314.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1289</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Wiklund, Mr. Karl Johan</td>\n",
       "      <td>male</td>\n",
       "      <td>21.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3101266</td>\n",
       "      <td>6.4958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1290</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>363272</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1291</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Willer, Mr. Aaron (\"Abi Weller\")</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3410</td>\n",
       "      <td>8.7125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1292</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Willey, Mr. Edward</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S.O./P.P. 751</td>\n",
       "      <td>7.5500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1293</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Williams, Mr. Howard Hugh \"Harry\"</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 2466</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Williams, Mr. Leslie</td>\n",
       "      <td>male</td>\n",
       "      <td>28.5000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>54636</td>\n",
       "      <td>16.1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1295</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Windelov, Mr. Einar</td>\n",
       "      <td>male</td>\n",
       "      <td>21.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SOTON/OQ 3101317</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Wirz, Mr. Albert</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315154</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>131.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1297</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Wiseman, Mr. Phillippe</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A/4. 34244</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Wittevrongel, Mr. Camille</td>\n",
       "      <td>male</td>\n",
       "      <td>36.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>345771</td>\n",
       "      <td>9.5000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1299</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Yasbeck, Mr. Antoni</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2659</td>\n",
       "      <td>14.4542</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Yasbeck, Mrs. Antoni (Selini Alexander)</td>\n",
       "      <td>female</td>\n",
       "      <td>15.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2659</td>\n",
       "      <td>14.4542</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1301</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Youseff, Mr. Gerious</td>\n",
       "      <td>male</td>\n",
       "      <td>45.5000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2628</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>312.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1302</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Yousif, Mr. Wazli</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2647</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1303</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Yousseff, Mr. Gerious</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2627</td>\n",
       "      <td>14.4583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Zabour, Miss. Hileni</td>\n",
       "      <td>female</td>\n",
       "      <td>14.5000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2665</td>\n",
       "      <td>14.4542</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>328.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Zabour, Miss. Thamine</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2665</td>\n",
       "      <td>14.4542</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Zakarian, Mr. Mapriededer</td>\n",
       "      <td>male</td>\n",
       "      <td>26.5000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2656</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>304.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Zakarian, Mr. Ortin</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2670</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Zimmerman, Mr. Leo</td>\n",
       "      <td>male</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315082</td>\n",
       "      <td>7.8750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1309 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pclass  survived                                               name  \\\n",
       "0          1         1                      Allen, Miss. Elisabeth Walton   \n",
       "1          1         1                     Allison, Master. Hudson Trevor   \n",
       "2          1         0                       Allison, Miss. Helen Loraine   \n",
       "3          1         0               Allison, Mr. Hudson Joshua Creighton   \n",
       "4          1         0    Allison, Mrs. Hudson J C (Bessie Waldo Daniels)   \n",
       "5          1         1                                Anderson, Mr. Harry   \n",
       "6          1         1                  Andrews, Miss. Kornelia Theodosia   \n",
       "7          1         0                             Andrews, Mr. Thomas Jr   \n",
       "8          1         1      Appleton, Mrs. Edward Dale (Charlotte Lamson)   \n",
       "9          1         0                            Artagaveytia, Mr. Ramon   \n",
       "10         1         0                             Astor, Col. John Jacob   \n",
       "11         1         1  Astor, Mrs. John Jacob (Madeleine Talmadge Force)   \n",
       "12         1         1                      Aubart, Mme. Leontine Pauline   \n",
       "13         1         1                       Barber, Miss. Ellen \"Nellie\"   \n",
       "14         1         1               Barkworth, Mr. Algernon Henry Wilson   \n",
       "15         1         0                                Baumann, Mr. John D   \n",
       "16         1         0                           Baxter, Mr. Quigg Edmond   \n",
       "17         1         1    Baxter, Mrs. James (Helene DeLaudeniere Chaput)   \n",
       "18         1         1                              Bazzani, Miss. Albina   \n",
       "19         1         0                               Beattie, Mr. Thomson   \n",
       "20         1         1                      Beckwith, Mr. Richard Leonard   \n",
       "21         1         1   Beckwith, Mrs. Richard Leonard (Sallie Monypeny)   \n",
       "22         1         1                              Behr, Mr. Karl Howell   \n",
       "23         1         1                              Bidois, Miss. Rosalie   \n",
       "24         1         1                                  Bird, Miss. Ellen   \n",
       "25         1         0                                Birnbaum, Mr. Jakob   \n",
       "26         1         1                            Bishop, Mr. Dickinson H   \n",
       "27         1         1            Bishop, Mrs. Dickinson H (Helen Walton)   \n",
       "28         1         1                             Bissette, Miss. Amelia   \n",
       "29         1         1          Bjornstrom-Steffansson, Mr. Mauritz Hakan   \n",
       "...      ...       ...                                                ...   \n",
       "1279       3         0               Vestrom, Miss. Hulda Amanda Adolfina   \n",
       "1280       3         0                                    Vovk, Mr. Janko   \n",
       "1281       3         0                               Waelens, Mr. Achille   \n",
       "1282       3         0                                Ware, Mr. Frederick   \n",
       "1283       3         0                        Warren, Mr. Charles William   \n",
       "1284       3         0                                  Webber, Mr. James   \n",
       "1285       3         0                                Wenzel, Mr. Linhart   \n",
       "1286       3         1    Whabee, Mrs. George Joseph (Shawneene Abi-Saab)   \n",
       "1287       3         0                   Widegren, Mr. Carl/Charles Peter   \n",
       "1288       3         0                          Wiklund, Mr. Jakob Alfred   \n",
       "1289       3         0                            Wiklund, Mr. Karl Johan   \n",
       "1290       3         1                   Wilkes, Mrs. James (Ellen Needs)   \n",
       "1291       3         0                   Willer, Mr. Aaron (\"Abi Weller\")   \n",
       "1292       3         0                                 Willey, Mr. Edward   \n",
       "1293       3         0                  Williams, Mr. Howard Hugh \"Harry\"   \n",
       "1294       3         0                               Williams, Mr. Leslie   \n",
       "1295       3         0                                Windelov, Mr. Einar   \n",
       "1296       3         0                                   Wirz, Mr. Albert   \n",
       "1297       3         0                             Wiseman, Mr. Phillippe   \n",
       "1298       3         0                          Wittevrongel, Mr. Camille   \n",
       "1299       3         0                                Yasbeck, Mr. Antoni   \n",
       "1300       3         1            Yasbeck, Mrs. Antoni (Selini Alexander)   \n",
       "1301       3         0                               Youseff, Mr. Gerious   \n",
       "1302       3         0                                  Yousif, Mr. Wazli   \n",
       "1303       3         0                              Yousseff, Mr. Gerious   \n",
       "1304       3         0                               Zabour, Miss. Hileni   \n",
       "1305       3         0                              Zabour, Miss. Thamine   \n",
       "1306       3         0                          Zakarian, Mr. Mapriededer   \n",
       "1307       3         0                                Zakarian, Mr. Ortin   \n",
       "1308       3         0                                 Zimmerman, Mr. Leo   \n",
       "\n",
       "         sex      age  sibsp  parch            ticket      fare    cabin  \\\n",
       "0     female  29.0000      0      0             24160  211.3375       B5   \n",
       "1       male   0.9167      1      2            113781  151.5500  C22 C26   \n",
       "2     female   2.0000      1      2            113781  151.5500  C22 C26   \n",
       "3       male  30.0000      1      2            113781  151.5500  C22 C26   \n",
       "4     female  25.0000      1      2            113781  151.5500  C22 C26   \n",
       "5       male  48.0000      0      0             19952   26.5500      E12   \n",
       "6     female  63.0000      1      0             13502   77.9583       D7   \n",
       "7       male  39.0000      0      0            112050    0.0000      A36   \n",
       "8     female  53.0000      2      0             11769   51.4792     C101   \n",
       "9       male  71.0000      0      0          PC 17609   49.5042      NaN   \n",
       "10      male  47.0000      1      0          PC 17757  227.5250  C62 C64   \n",
       "11    female  18.0000      1      0          PC 17757  227.5250  C62 C64   \n",
       "12    female  24.0000      0      0          PC 17477   69.3000      B35   \n",
       "13    female  26.0000      0      0             19877   78.8500      NaN   \n",
       "14      male  80.0000      0      0             27042   30.0000      A23   \n",
       "15      male      NaN      0      0          PC 17318   25.9250      NaN   \n",
       "16      male  24.0000      0      1          PC 17558  247.5208  B58 B60   \n",
       "17    female  50.0000      0      1          PC 17558  247.5208  B58 B60   \n",
       "18    female  32.0000      0      0             11813   76.2917      D15   \n",
       "19      male  36.0000      0      0             13050   75.2417       C6   \n",
       "20      male  37.0000      1      1             11751   52.5542      D35   \n",
       "21    female  47.0000      1      1             11751   52.5542      D35   \n",
       "22      male  26.0000      0      0            111369   30.0000     C148   \n",
       "23    female  42.0000      0      0          PC 17757  227.5250      NaN   \n",
       "24    female  29.0000      0      0          PC 17483  221.7792      C97   \n",
       "25      male  25.0000      0      0             13905   26.0000      NaN   \n",
       "26      male  25.0000      1      0             11967   91.0792      B49   \n",
       "27    female  19.0000      1      0             11967   91.0792      B49   \n",
       "28    female  35.0000      0      0          PC 17760  135.6333      C99   \n",
       "29      male  28.0000      0      0            110564   26.5500      C52   \n",
       "...      ...      ...    ...    ...               ...       ...      ...   \n",
       "1279  female  14.0000      0      0            350406    7.8542      NaN   \n",
       "1280    male  22.0000      0      0            349252    7.8958      NaN   \n",
       "1281    male  22.0000      0      0            345767    9.0000      NaN   \n",
       "1282    male      NaN      0      0            359309    8.0500      NaN   \n",
       "1283    male      NaN      0      0        C.A. 49867    7.5500      NaN   \n",
       "1284    male      NaN      0      0  SOTON/OQ 3101316    8.0500      NaN   \n",
       "1285    male  32.5000      0      0            345775    9.5000      NaN   \n",
       "1286  female  38.0000      0      0              2688    7.2292      NaN   \n",
       "1287    male  51.0000      0      0            347064    7.7500      NaN   \n",
       "1288    male  18.0000      1      0           3101267    6.4958      NaN   \n",
       "1289    male  21.0000      1      0           3101266    6.4958      NaN   \n",
       "1290  female  47.0000      1      0            363272    7.0000      NaN   \n",
       "1291    male      NaN      0      0              3410    8.7125      NaN   \n",
       "1292    male      NaN      0      0     S.O./P.P. 751    7.5500      NaN   \n",
       "1293    male      NaN      0      0          A/5 2466    8.0500      NaN   \n",
       "1294    male  28.5000      0      0             54636   16.1000      NaN   \n",
       "1295    male  21.0000      0      0  SOTON/OQ 3101317    7.2500      NaN   \n",
       "1296    male  27.0000      0      0            315154    8.6625      NaN   \n",
       "1297    male      NaN      0      0        A/4. 34244    7.2500      NaN   \n",
       "1298    male  36.0000      0      0            345771    9.5000      NaN   \n",
       "1299    male  27.0000      1      0              2659   14.4542      NaN   \n",
       "1300  female  15.0000      1      0              2659   14.4542      NaN   \n",
       "1301    male  45.5000      0      0              2628    7.2250      NaN   \n",
       "1302    male      NaN      0      0              2647    7.2250      NaN   \n",
       "1303    male      NaN      0      0              2627   14.4583      NaN   \n",
       "1304  female  14.5000      1      0              2665   14.4542      NaN   \n",
       "1305  female      NaN      1      0              2665   14.4542      NaN   \n",
       "1306    male  26.5000      0      0              2656    7.2250      NaN   \n",
       "1307    male  27.0000      0      0              2670    7.2250      NaN   \n",
       "1308    male  29.0000      0      0            315082    7.8750      NaN   \n",
       "\n",
       "     embarked boat   body                           home.dest  \n",
       "0           S    2    NaN                        St Louis, MO  \n",
       "1           S   11    NaN     Montreal, PQ / Chesterville, ON  \n",
       "2           S  NaN    NaN     Montreal, PQ / Chesterville, ON  \n",
       "3           S  NaN  135.0     Montreal, PQ / Chesterville, ON  \n",
       "4           S  NaN    NaN     Montreal, PQ / Chesterville, ON  \n",
       "5           S    3    NaN                        New York, NY  \n",
       "6           S   10    NaN                          Hudson, NY  \n",
       "7           S  NaN    NaN                         Belfast, NI  \n",
       "8           S    D    NaN                 Bayside, Queens, NY  \n",
       "9           C  NaN   22.0                 Montevideo, Uruguay  \n",
       "10          C  NaN  124.0                        New York, NY  \n",
       "11          C    4    NaN                        New York, NY  \n",
       "12          C    9    NaN                       Paris, France  \n",
       "13          S    6    NaN                                 NaN  \n",
       "14          S    B    NaN                       Hessle, Yorks  \n",
       "15          S  NaN    NaN                        New York, NY  \n",
       "16          C  NaN    NaN                        Montreal, PQ  \n",
       "17          C    6    NaN                        Montreal, PQ  \n",
       "18          C    8    NaN                                 NaN  \n",
       "19          C    A    NaN                        Winnipeg, MN  \n",
       "20          S    5    NaN                        New York, NY  \n",
       "21          S    5    NaN                        New York, NY  \n",
       "22          C    5    NaN                        New York, NY  \n",
       "23          C    4    NaN                                 NaN  \n",
       "24          S    8    NaN                                 NaN  \n",
       "25          C  NaN  148.0                   San Francisco, CA  \n",
       "26          C    7    NaN                        Dowagiac, MI  \n",
       "27          C    7    NaN                        Dowagiac, MI  \n",
       "28          S    8    NaN                                 NaN  \n",
       "29          S    D    NaN  Stockholm, Sweden / Washington, DC  \n",
       "...       ...  ...    ...                                 ...  \n",
       "1279        S  NaN    NaN                                 NaN  \n",
       "1280        S  NaN    NaN                                 NaN  \n",
       "1281        S  NaN    NaN      Antwerp, Belgium / Stanton, OH  \n",
       "1282        S  NaN    NaN                                 NaN  \n",
       "1283        S  NaN    NaN                                 NaN  \n",
       "1284        S  NaN    NaN                                 NaN  \n",
       "1285        S  NaN  298.0                                 NaN  \n",
       "1286        C    C    NaN                                 NaN  \n",
       "1287        S  NaN    NaN                                 NaN  \n",
       "1288        S  NaN  314.0                                 NaN  \n",
       "1289        S  NaN    NaN                                 NaN  \n",
       "1290        S  NaN    NaN                                 NaN  \n",
       "1291        S  NaN    NaN                                 NaN  \n",
       "1292        S  NaN    NaN                                 NaN  \n",
       "1293        S  NaN    NaN                                 NaN  \n",
       "1294        S  NaN   14.0                                 NaN  \n",
       "1295        S  NaN    NaN                                 NaN  \n",
       "1296        S  NaN  131.0                                 NaN  \n",
       "1297        S  NaN    NaN                                 NaN  \n",
       "1298        S  NaN    NaN                                 NaN  \n",
       "1299        C    C    NaN                                 NaN  \n",
       "1300        C  NaN    NaN                                 NaN  \n",
       "1301        C  NaN  312.0                                 NaN  \n",
       "1302        C  NaN    NaN                                 NaN  \n",
       "1303        C  NaN    NaN                                 NaN  \n",
       "1304        C  NaN  328.0                                 NaN  \n",
       "1305        C  NaN    NaN                                 NaN  \n",
       "1306        C  NaN  304.0                                 NaN  \n",
       "1307        C  NaN    NaN                                 NaN  \n",
       "1308        S  NaN    NaN                                 NaN  \n",
       "\n",
       "[1309 rows x 14 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(filename=DATA_FOLDER+'/titanic.html')\n",
    "\n",
    "## Import Excel file titanic.xls\n",
    "data3 = pd.read_excel(DATA_FOLDER + '/titanic.xls','titanic', na_values=['NA']) # size 1309 x 14\n",
    "data3"
>>>>>>> 1b262922053d0e529ee7500f49e40599fe52f4d1
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the following questions state clearly your assumptions and discuss your findings:\n",
    "1. Describe the *type* and the *value range* of each attribute. Indicate and transform the attributes that can be `Categorical`. \n",
    "2. Plot histograms for the *travel class*, *embarkation port*, *sex* and *age* attributes. For the latter one, use *discrete decade intervals*. \n",
    "3. Calculate the proportion of passengers by *cabin floor*. Present your results in a *pie chart*.\n",
    "4. For each *travel class*, calculate the proportion of the passengers that survived. Present your results in *pie charts*.\n",
    "5. Calculate the proportion of the passengers that survived by *travel class* and *sex*. Present your results in *a single histogram*.\n",
    "6. Create 2 equally populated *age categories* and calculate survival proportions by *age category*, *travel class* and *sex*. Present your results in a `DataFrame` with unique index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of each attribute is shown in the table below : \n",
      "pclass         int64\n",
      "survived       int64\n",
      "name          object\n",
      "sex           object\n",
      "age          float64\n",
      "sibsp          int64\n",
      "parch          int64\n",
      "ticket        object\n",
      "fare         float64\n",
      "cabin         object\n",
      "embarked      object\n",
      "boat          object\n",
      "body         float64\n",
      "home.dest     object\n",
      "dtype: object\n",
      "            pclass     survived          age        sibsp        parch  \\\n",
      "count  1309.000000  1309.000000  1046.000000  1309.000000  1309.000000   \n",
      "mean      2.294882     0.381971    29.881135     0.498854     0.385027   \n",
      "std       0.837836     0.486055    14.413500     1.041658     0.865560   \n",
      "min       1.000000     0.000000     0.166700     0.000000     0.000000   \n",
      "25%       2.000000     0.000000    21.000000     0.000000     0.000000   \n",
      "50%       3.000000     0.000000    28.000000     0.000000     0.000000   \n",
      "75%       3.000000     1.000000    39.000000     1.000000     0.000000   \n",
      "max       3.000000     1.000000    80.000000     8.000000     9.000000   \n",
      "\n",
      "              fare        body  \n",
      "count  1308.000000  121.000000  \n",
      "mean     33.295479  160.809917  \n",
      "std      51.758668   97.696922  \n",
      "min       0.000000    1.000000  \n",
      "25%       7.895800   72.000000  \n",
      "50%      14.454200  155.000000  \n",
      "75%      31.275000  256.000000  \n",
      "max     512.329200  328.000000  \n"
     ]
    }
   ],
   "source": [
    "## -- 1 -- ## \n",
    "print('The type of each attribute is shown in the table below : ', end='\\n')\n",
    "print(data3.dtypes)\n",
    "\n",
    "\n",
    "print(data3.describe())\n",
    " \n",
    "\n",
    "print?\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
<<<<<<< HEAD
   "source": [
    "## Import Excel file titanic.xls\n",
    "titanic_data = pd.read_excel(DATA_FOLDER + '/titanic.xls') # size 1309 x 14\n",
    "titanic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning data ##\n",
    "# Before starting, one needs to clean the data and do some data transformations\n",
    "\n",
    "# First check NaN values in the data\n",
    "titanic_data.isnull().any(); \n",
    "# This line told us what attributes get NaN values. As one can see only attribute for age, fare, cabin,embarked,boat,body and home.dest have Nan Values\n",
    "# Now that we know which attributes have Nan Values, it is good to check the number of Nan Values for each attribute.\n",
    "titanic_data.isnull().sum()\n",
    "# One can see that cabin, boat and body have significant amounts of NaN value, therefore we cannot drop the entire row when there are NaN values for these attributes\n",
    "# The number of NaN values specially for boat and cabin attributes are not the same as in the HTML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- 1 -- ## \n",
    "print('The type of each attribute is shown in the table below : ', end='\\n')\n",
    "print(titanic_data.dtypes) # Showing the type of each attribute\n",
    "print('The value range of each attribute is shown in the table below : ', end='\\n\\n')\n",
    "print(titanic_data.describe())\n",
    "\n",
    "titanic_data['boat'] = titanic_data.boat.replace({'C D' : np.nan, '5 7': np.nan, '13 15': np.nan , '15 16' : np.nan, '5 9': np.nan, '13 15 B':np.nan, '8 10': np.nan }) # Change the value when there are multiple life boat number and put a Nan Value because when cannot know in which boat the passenger was in.\n",
    "categorical_map = np.array(['pclass','survived','sex','embarked','boat','sibsp','parch']) # List of attributes that can be categorical (see discussion below for more informations)\n",
    "     \n",
    "for x in categorical_map : # This loop transforms the attributes that can be categorical.\n",
    "    titanic_data[x] = titanic_data[x].astype('category')\n",
    "    print(titanic_data[x].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion 1 :\n",
    "- Regarding the type table, one can notice that the age's type is float64 but it could make more sense. However if it was in int64 but if the age was determine by the birth date then it makes sense\n",
    "- Besides, one can notice that the types of the ticket number, life boat number and the cabin number are object because there are some letters in the values\n",
    "\n",
    "- In the table, the value range is determine by [min,max]. One cant notice that there were babies and old person on the boat and the number of body identification is equal to 328 which is approximatively the number of body found. Also, some passengers didn't pay their ticket (free ticket)\n",
    "\n",
    "- Categoricals are pandas datatype : a variable, which can take on only a limited and usually fixed, number of possible values\n",
    "- Assumptions : boat = life boat according to us, we can put Nan Value for the passengers that have more than one value because unlike the cabine where a passenger could reserve more than 1 cabin a passenger couldn't have been in 2 different boat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- 2 -- ##\n",
    "# Plot 4 histograms using plot with kind = 'bar' and value_counts\n",
    "plt.subplot(141)\n",
    "titanic_data['pclass'].value_counts().plot(title = 'Travel class', kind='bar', grid = True) \n",
    "plt.subplot(142)\n",
    "titanic_data['sex'].value_counts().plot(title = 'Gender', kind='bar', grid = True)\n",
    "plt.subplot(143)\n",
    "titanic_data['embarked'].value_counts().plot(title = 'Embarkation port', kind='bar', grid = True)\n",
    "plt.subplot(144)\n",
    "age_noNA = titanic_data['age'].dropna() # Drop NaN values to make sure they didn't bias the result\n",
    "pd.cut(age_noNA,[0,10,20,30,40,50,60,70,80]).value_counts().plot(title = 'age', kind='bar', grid = True) #We don't need more than 80 years old since the maximun age is 80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion 2 :\n",
    "- Regarding the histograms, one can notice that there were more passenger travelling in Third class\n",
    "- There were more male than female, almost twice\n",
    "- Majority of the passengers embarked on port S\n",
    "- One can notice that there were a lot of young passenger between 0 and 50 years old. 20 - 30 years old passengers were the greatest number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- 3 -- ##\n",
    "\n",
    "titanic_data['cabin'] = titanic_data.cabin.replace({'T': np.nan }) # Drop the T value (see discussion below for more informations)\n",
    "titanic_data['cabin_floor'] = titanic_data.cabin.copy().str[0] # Taking only the first letter which corresponds to the Floor\n",
    "titanic_data['cabin_floor'] = titanic_data.cabin_floor.astype('category') # Create category for each floor\n",
    "floor = np.zeros(len(titanic_data.cabin_floor.cat.categories))\n",
    "for i,x in enumerate(titanic_data.cabin_floor.cat.categories) : # Loop to calculate the proportion for each Cabin floor\n",
    "    floor[i] = titanic_data['cabin_floor'].value_counts()[i]*100/titanic_data['cabin_floor'].value_counts().sum() # Number of passenger of each floor divided by the number of passenger for which we know on what floor they were. \n",
    "    print('The percentage of passenger on Cabin Floor ' + x + ' is equal to %f percent' % floor[i]) \n",
    "    i = i+1\n",
    "    \n",
    "titanic_data['cabin_floor'].value_counts().plot(title = 'Cabin Floor', kind='pie') # Plot the proportion in a pie chart\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion 3 :\n",
    "-  Assumptions : regarding the data and the HTML, passengers which have more than one cabine are on the same cabin floor\n",
    "- Besides, regarding the map of all the deck of the titanic, one can notice that one floor F there are different section such as G or E, so for instance the cabin F EXX is on the F floor.\n",
    "- The cabin T could be a typo or this cabin exist but since we don't know on which floor the cabin is, we just replace the value by NaN value.\n",
    "- Assumptions : To calculate the proportion we are taking the number of passenger on each floor divided by the total number of passenger that we know on which floor they were, and not the number total of passengers\n",
    "- One can notice that the floor B,C and D were the most occupied which makes sense since this floor corresponds to third and second class which were the greatest number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- 4 -- ##\n",
    "titanic_data.survived.cat.categories = ['Dead', 'Survived']\n",
    "pclass_grouped = titanic_data.groupby(titanic_data.pclass) # Group the passenger in each travel class\n",
    "class_dict = dict(list(pclass_grouped))\n",
    "\n",
    "# Calculate proportion\n",
    "class1 = class_dict[1].survived.value_counts()[1]*100/(class_dict[1].survived.value_counts().sum())\n",
    "class2 = class_dict[2].survived.value_counts()[1]*100/(class_dict[2].survived.value_counts().sum())\n",
    "class3 = class_dict[3].survived.value_counts()[1]*100/(class_dict[3].survived.value_counts().sum())\n",
    "\n",
    "print('The proportion of the passengers that survived for class 1 is : %f percent' % class1 )\n",
    "print('The proportion of the passengers that survived for class 2 is : %f percent' % class2 )\n",
    "print('The proportion of the passengers that survived for class 3 is : %f percent' % class3 )\n",
    "\n",
    "# Pie charts\n",
    "plt.subplot(131)\n",
    "class_dict[1].survived.value_counts().plot(title = 'Class 1 Survival', kind='pie')\n",
    "plt.subplot(132)\n",
    "class_dict[2].survived.value_counts().plot(title = 'Class 2 Survival', kind='pie')\n",
    "plt.subplot(133)\n",
    "class_dict[3].survived.value_counts().plot(title = 'Class 3 Survival', kind='pie') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion 4 :\n",
    "- One can notice that the proportion of passenger that survived in First class is more important that the 2 others. And the class with the lower proportion of survival is the Third class which makes sense since they were on the floors that were lower and therefore were far from the life boat and first affected by the sinking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- 5 -- ##\n",
    "f_sur = np.zeros(len(titanic_data.pclass.cat.categories))\n",
    "m_sur = np.zeros(len(titanic_data.pclass.cat.categories))\n",
    "sex_class_grouped = titanic_data.groupby(['pclass','sex']) # Group the passenger by travel class and sex\n",
    "sex_class_dict = dict(list(sex_class_grouped))\n",
    "\n",
    "for i,x in enumerate(titanic_data.pclass.cat.categories): # Loop that calculate the proportion of survival for each travel class and sex\n",
    "    f_sur[i] = sex_class_dict[x,'female'].survived.value_counts()[1]*100/sex_class_dict[x,'female'].survived.value_counts().sum()\n",
    "    m_sur[i] = sex_class_dict[x,'male'].survived.value_counts()[1]*100/sex_class_dict[x,'male'].survived.value_counts().sum()\n",
    "    print('For Class' + str(x))\n",
    "    print('The percentage of passenger female that survived is : %f' % f_sur[i] +'percent')\n",
    "    print('The percentage of passenger male that survived is : %f' % m_sur[i] +'percent')\n",
    "    \n",
    "# Create a single histogram which represents the proportion of survival for each travel class and sex\n",
    "travel = [1,2,3]\n",
    "bar_width = 0.2\n",
    "x = np.arange(len(titanic_data.pclass.cat.categories))\n",
    "opacity = 0.5\n",
    "plt.bar(x,f_sur,bar_width, color ='green', label = 'Female', alpha = opacity)\n",
    "plt.bar(x + bar_width ,m_sur ,bar_width, color ='orange', label = 'Male', alpha = opacity)\n",
    "plt.legend()\n",
    "plt.xlabel('Travel Class')\n",
    "plt.ylabel('Percentage of Survival')\n",
    "plt.title('Survival by travel class and sex')\n",
    "plt.xticks(x +bar_width, travel)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion 5 :\n",
    "- One can notice that the proportion of female that survived is higher that for male and this for each travel class. Besides the highest proportion of female that survived is in First class and lowest is in the Third Class\n",
    "- One can notice that the proportion of male that survived in general is much lower that for female which makes sense because during a sinking the children and women are the first saved (Children and Ladies First !). And again the proportion of survival is higher in First class than the 2 other travel class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- 6 -- ##\n",
    "Pop_1 = age_noNA.sort_values()[:523] # Create 2 equally populated age categories by sorting the age's values and taking the first 523 one since the number of age's value is 1046\n",
    "Pop_2 = age_noNA.sort_values()[524:]\n",
    "\n",
    "f_old_sur = np.zeros(len(titanic_data.pclass.cat.categories))\n",
    "m_old_sur = np.zeros(len(titanic_data.pclass.cat.categories))\n",
    "f_young_sur = np.zeros(len(titanic_data.pclass.cat.categories))\n",
    "m_young_sur = np.zeros(len(titanic_data.pclass.cat.categories))\n",
    "\n",
    "# Add new column in the dataframe with value 'Young' for the first group and 'Old' for the second group\n",
    "\n",
    "population1 = pd.Series(['Young']*len(Pop_1.index), Pop_1.index) \n",
    "titanic_data['eq_pop'] = population1\n",
    "titanic_data['eq_pop'] = titanic_data['eq_pop'].fillna(value ='Old')\n",
    "titanic_data['eq_pop'] = titanic_data['eq_pop'].astype('category')\n",
    "titanic_data.eq_pop.cat.categories = ['Old', 'Young']\n",
    "\n",
    "sex_class_survived_grouped = titanic_data.groupby(['pclass','sex','survived']) # Group the passengers by travel class, sex and survival\n",
    "sex_class_survived_dict = dict(list(sex_class_survived_grouped))\n",
    "\n",
    "for i,x in enumerate(titanic_data.pclass.cat.categories):\n",
    "    f_young_sur[i] = sex_class_survived_dict[x,'female','Survived'].eq_pop.value_counts()[1]*100/sex_class_dict[x,'female'].survived.value_counts().sum()\n",
    "    f_old_sur[i] = sex_class_survived_dict[x,'female','Survived'].eq_pop.value_counts()[0]*100/sex_class_dict[x,'female'].survived.value_counts().sum()\n",
    "    m_young_sur[i] = sex_class_survived_dict[x,'male','Survived'].eq_pop.value_counts()[1]*100/sex_class_dict[x,'male'].survived.value_counts().sum()\n",
    "    m_old_sur[i] = sex_class_survived_dict[x,'male','Survived'].eq_pop.value_counts()[0]*100/sex_class_dict[x,'male'].survived.value_counts().sum()\n",
    "    print('For Class' + str(x))\n",
    "    print('The percentage of passenger young female that survived is : %f' % f_young_sur[i] +'percent')\n",
    "    print('The percentage of passenger old female that survived is : %f' % f_old_sur[i] +'percent')\n",
    "    print('The percentage of passenger young male that survived is : %f' % m_young_sur[i] +'percent')\n",
    "    print('The percentage of passenger old male that survived is : %f' % m_old_sur[i]+'percent')\n",
    "\n",
    "# Create one histogram that represents the proportion of survival depending on the travel class, sex, and the age's group\n",
    "travel = [1,2,3]\n",
    "bar_width = 0.2\n",
    "x = np.arange(len(titanic_data.pclass.cat.categories))\n",
    "opacity = 0.5\n",
    "plt.bar(x,f_young_sur,bar_width, color ='green', label = 'Young Female', alpha = opacity)\n",
    "plt.bar(x,f_old_sur,bar_width, color ='blue', label = 'Old Female', alpha = opacity, bottom = f_young_sur)\n",
    "plt.bar(x + bar_width ,m_young_sur ,bar_width, color ='orange', label = 'Young Male', alpha = opacity)\n",
    "plt.bar(x + bar_width ,m_old_sur ,bar_width, color ='red', label = 'Old Male', alpha = opacity, bottom = m_young_sur)\n",
    "plt.legend()\n",
    "plt.xlabel('Travel Class')\n",
    "plt.ylabel('Percentage of Survival')\n",
    "plt.title('Survival by travel class, sex and age')\n",
    "plt.xticks(x +bar_width, travel)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion 6 :\n",
    "- One can notice that for both male and female the proportion of passenger who belong to the second that survived is higher than the other group for the First class unlike the 2 other travel class where the first group have a higher proportion of survival. \n",
    "- Besides, the difference between the 2 groups in much more important for the first class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General conclusion :\n",
    "- Regarding the proportion of passengers that survived. One can notice that there were a huge inegality of survival between the different travel class. Indeed, although the passenger of the second and third class were on lower decks so far from the life boats and the first affected by the water in the boat, we remembered that some of the rich passengers paid some amounts of money to get in the life boats.\n",
    "- Besides, it makes sense that the proportion of women that survived is higher.\n",
    "\n",
    "In conclusion, during the sinking of the Titanic, a class war really took place and the first class (rich people) won (at least for the women). However, we will remember the courtesy of the men who saved the children and women first (as we can see that the proportion of women that survived is much higher)"
   ]
=======
   "source": []
>>>>>>> 1b262922053d0e529ee7500f49e40599fe52f4d1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
